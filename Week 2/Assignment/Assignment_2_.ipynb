{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Collection and Loading**\n",
        "\n",
        "DailyMail News Text Summarization\n",
        "[Link](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail).\n",
        "\n"
      ],
      "metadata": {
        "id": "K003mqp9buk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc3neBuqdyZX",
        "outputId": "8c8524ea-8d5b-4727-be4f-7df5c63bd174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n"
      ],
      "metadata": {
        "id": "6LIfBpHCkpzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your-file-path.csv' with the actual path to your CSV file\n",
        "file_path = '/content/drive/My Drive/Sijin/Data Set.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Xvor4EkuKN",
        "outputId": "3fbbfc2f-f906-4ee1-e711-9c9ad3f980fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "5  5ed5e3fbd235a8046cd3b87f4a1aa51b856c8ec3   \n",
            "6  6394f51b120ceb3da5e7b53dd5167fc4cf80b514   \n",
            "7  98be9b2d558c17df8a13597195957a7c8587ddcd   \n",
            "8  57f68638739c3a1de8d9922b389d6ded39977012   \n",
            "9  20778c35c19d741cc182719de336d71e1a0b228e   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "5  This is the moment that a crew of firefighters...   \n",
            "6  The amount of time people spend listening to B...   \n",
            "7  (CNN)So, you'd like a \"Full House\" reunion and...   \n",
            "8  At 11:20pm, former world champion Ken Doherty ...   \n",
            "9  A gang of six men have been jailed for a total...   \n",
            "\n",
            "                                          highlights  \n",
            "0  Experts question if  packed out planes are put...  \n",
            "1  Drunk teenage boy climbed into lion enclosure ...  \n",
            "2  Nottingham Forest are close to extending Dougi...  \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
            "4  Tell-all interview with the reality TV star, 6...  \n",
            "5  Giant pig fell into the swimming pool at his h...  \n",
            "6  Figures show that while millions still tune in...  \n",
            "7  Show will return with a one-hour special, foll...  \n",
            "8  Reanne Evans faced Ken Doherty in World Champi...  \n",
            "9  Gang have been jailed for a total of 31 years ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x5aFJwWDnikX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Text Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wztKIMpYnnSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhJp5-RgoM9B",
        "outputId": "2e58f959-f6cb-493c-acc3-3e8e79cdc5de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6J8gUV8oMxn",
        "outputId": "4ad036a0-e744-4e3c-de7b-d590b7a5cd8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Choose a text corpus from the NLTK library, which is publicly available and widely used in text processing.\n"
      ],
      "metadata": {
        "id": "NQQcJU_En6hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# List all files in the Gutenberg corpus\n",
        "file_ids = gutenberg.fileids()\n",
        "print(\"Available Texts in Gutenberg Corpus:\", file_ids)\n",
        "\n",
        "# Choose a specific text\n",
        "chosen_text = 'austen-emma.txt'\n",
        "\n",
        "# Load the text\n",
        "text = gutenberg.raw(chosen_text)\n",
        "\n",
        "# Display the first few lines of the text\n",
        "print(\"\\nFirst 500 characters of the chosen text:\\n\")\n",
        "print(text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEBQqje_oGH6",
        "outputId": "d9fa02ee-1198-4fc9-bece-6b19c67cfa3c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Texts in Gutenberg Corpus: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "First 500 characters of the chosen text:\n",
            "\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n"
      ],
      "metadata": {
        "id": "Q8d4oLw5sLea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7d0NPNSsk6W",
        "outputId": "87617cdb-b6ad-4256-e51c-454ea6e4867a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the text\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "1dLRNsvrscdN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some of the tokens\n",
        "print(\"Text :\", text[:500])\n",
        "print(\"First 10 words:\", words[:20])\n",
        "print(\"First 1 sentence:\", sentences[:1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYWJg08OsipC",
        "outputId": "5ba987b5-4672-481f-8b8e-c69588ea9de6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text : [Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "First 10 words: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
            "First 1 sentence: ['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "_q7Yd4c30vTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem the words\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Display some of the stemmed words\n",
        "print(\"First 10 stemmed words:\", stemmed_words[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asP1DGy30zTV",
        "outputId": "db71a5ce-41c0-4072-9374-4b268437fb3d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 stemmed words: ['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizatiom"
      ],
      "metadata": {
        "id": "0n1srBPP1WWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Display some of the lemmatized words\n",
        "print(\"First 10 lemmatized words:\", lemmatized_words[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUpmtX1d1aHj",
        "outputId": "942873bc-02f4-4b85-e2bc-6fd1299d8418"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 lemmatized words: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop words Removal"
      ],
      "metadata": {
        "id": "c9PM7c3S1gL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get the list of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the lemmatized words\n",
        "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
        "\n",
        "# Display some of the filtered words\n",
        "print(\"First 10 words after stop word removal:\", filtered_words[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-7u6kyN2HZX",
        "outputId": "0ed64fd5-5561-4572-ad54-a42ff403bc74"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words after stop word removal: ['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Feature Extraction Techniques**\n"
      ],
      "metadata": {
        "id": "Sm5UPKu63DML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "tttIB3GK3GUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assume we have processed_sentences from Part 2:\n",
        "# List of processed sentences where stop words and non-alphabetic tokens have been removed\n",
        "\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the processed sentences to create the Bag-of-Words model\n",
        "X = vectorizer.fit_transform(filtered_words)\n",
        "\n",
        "# Display the feature names (words) and the Bag-of-Words representation\n",
        "print(\"Feature names (first 10):\", vectorizer.get_feature_names_out()[:10])\n",
        "print(\"Bag-of-Words representation (first 5 sentences):\\n\", X.toarray()[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJW4gKI3tcl",
        "outputId": "d8d31b81-10a2-4530-c00d-e73983d2efff"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names (first 10): ['000' '10' '1816' '23rd' '24th' '26th' '28th' '7th' '8th' '_______']\n",
            "Bag-of-Words representation (first 5 sentences):\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UOi2bA722NN6"
      }
    }
  ]
}